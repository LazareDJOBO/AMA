# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x2tROKS3vAc8qTrFz4N7PieVbRn-gSsO
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

path = "/content/drive/MyDrive/Data AI4CKD.xlsx"
df = pd.read_excel(path)

df.head()

cols_to_drop = ["ID"]

# Colonnes post-diagnostic √† exclure
cols_causes = [
    col for col in df.columns
    if col.startswith("Causes Majeure apr√®s Diagnostic/")
]

cols_to_drop.extend(cols_causes)

# Colonnes exclues
cols_to_drop = ["ID"]

cols_causes = [
    col for col in df.columns
    if col.startswith("Causes Majeure apr√®s Diagnostic/")
]

cols_to_drop.extend(cols_causes)

# Affichage
print("Variables exclues du dataset :\n")
for col in cols_to_drop:
    print("-", col)

df_final = df.drop(columns=cols_to_drop)

df_final['Stage de l\'IRC'].unique()

import pandas as pd

# Pourcentage de cellules non nulles par colonne
non_null_percentage = df_final.notnull().sum() / len(df_final) * 100

# Afficher les 10 premi√®res colonnes pour v√©rifier
print(non_null_percentage.head)

non_null_percentage[non_null_percentage > 29]

import pandas as pd

# Calcul du pourcentage de cellules non nulles par colonne
non_null_percentage = df_final.notnull().sum() / len(df_final) * 100

# S√©lection des colonnes avec plus de 50% de donn√©es pr√©sentes
selected_columns = non_null_percentage[non_null_percentage > 29].index

# Cr√©ation d'un nouveau DataFrame avec ces colonnes
df_filtered = df_final[selected_columns]

# Colonnes √† supprimer
columns_to_drop = [
    "Causes Majeure apr√®s Diagnostic/HTA",
    "Causes Majeure apr√®s Diagnostic/Diab√®te ID",
    "Nationalit√©"
]

# Suppression des colonnes si elles existent dans df_filtered
df_filtered = df_filtered.drop(columns=[col for col in columns_to_drop if col in df_filtered.columns])

import pandas as pd

# D√©tecter automatiquement les colonnes num√©riques et cat√©gorielles
numeric_cols = df_filtered.select_dtypes(include=['number']).columns.tolist()
categorical_cols = df_filtered.select_dtypes(include=['object', 'category']).columns.tolist()

# Imputation pour les colonnes num√©riques (m√©diane)
for col in numeric_cols:
    df_filtered[col].fillna(df_filtered[col].median(), inplace=True)

# Imputation pour les colonnes cat√©gorielles (mode)
for col in categorical_cols:
    df_filtered[col].fillna(df_filtered[col].mode()[0], inplace=True)

# V√©rification des valeurs manquantes restantes
df_filtered.isnull().sum()

df_filtered.head(5)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ================================
# √âtape 1 : D√©tection automatique des types
# ================================

# Target
target_col = "Stage de l'IRC"

# Colonnes num√©riques
numeric_cols = df_filtered.select_dtypes(include=['number']).columns.tolist()

# Retirer la target si elle est num√©rique
if target_col in numeric_cols:
    numeric_cols.remove(target_col)

# D√©tection des colonnes binaires (0/1)
binary_cols = [col for col in numeric_cols
               if df_filtered[col].nunique() == 2]

# Retirer les binaires des num√©riques continues
numeric_cols = [col for col in numeric_cols if col not in binary_cols]

# Colonnes cat√©gorielles
categorical_cols = df_filtered.select_dtypes(include=['object', 'category']).columns.tolist()

# ================================
# √âtape 2 : Analyse Univari√©e
# ================================

# -------- 2a) Variables num√©riques continues --------
print("\n=== Analyse Univari√©e : Variables Num√©riques ===")

for col in numeric_cols:
    print(f"\nColonne : {col}")
    print(df_filtered[col].describe())

    plt.figure(figsize=(6,3))
    sns.histplot(df_filtered[col], kde=True)
    plt.title(f'Distribution de {col}')
    plt.show()


# -------- 2b) Variables cat√©gorielles --------
print("\n=== Analyse Univari√©e : Variables Cat√©gorielles ===")

for col in categorical_cols:
    print(f"\nColonne : {col}")
    print(df_filtered[col].value_counts(dropna=False))

    plt.figure(figsize=(6,3))
    sns.countplot(y=col, data=df_filtered,
                  order=df_filtered[col].value_counts().index)
    plt.title(f'Distribution de {col}')
    plt.show()


# -------- 2c) Variables binaires (0/1) --------
print("\n=== Analyse Univari√©e : Variables Binaires (0/1) ===")

for col in binary_cols:
    print(f"\nColonne : {col}")
    print(df_filtered[col].value_counts())

    plt.figure(figsize=(4,2))
    sns.countplot(x=col, data=df_filtered)
    plt.title(f'Distribution de {col}')
    plt.show()


# -------- 2d) Target --------
print("\n=== Analyse Univari√©e : Target Stage de l'IRC ===")

print(df_filtered[target_col].value_counts())

plt.figure(figsize=(6,3))
sns.countplot(x=target_col, data=df_filtered)
plt.title("Distribution des stades de l'IRC")
plt.show()

# ================================
# LISTE FINALE DES FEATURES
# ================================

features = [

# üîπ 1. D√©mographiques
"Age",
"Sexe",
"Poids (Kg)",
"Taille (m)",

# üîπ 2. Signes vitaux
"TA (mmHg)/Systole",
"TA (mmHg)/Diastole",
"Poul (bpm)",
"Temp√©rature (C¬∞)",
"Conscience",
"Score de Glasgow (/15)",

# üîπ 3. Biologie sanguine
"Cr√©atinine (mg/L)",
"Ur√©e (g/L)",
"Na^+ (meq/L)",
"K^+ (meq/L)",
"Cl^- (meq/L)",
"Ca^2+ (meq/L)",
"P (meq/L)",
"Glyc√©mie √† jeun (taux de Glucose)",
"Cholest√©rol Total",
"Cholest√©rol HDL",
"Cholest√©rol LDL",
"Triglyc√©rides",

# üîπ 4. H√©matologie
"Hb (g/dL)",
"Hte (%)",
"VGM (fL)",
"TCMH (pg)",
"CCMH (%)",
"NB (G/L)",
"Plaquettes (g/L)",
"An√©mie",

# üîπ 5. Urinaire
"Diur√®se",
"Tests Populaires/PU (g/24h)",

# üîπ 6. Echographie
"Grosseur Rein Gauche ",
"Grosseur Rein Droit ",
"Diff√©renciation des reins",
"Echog√©nicit√©",
"Contour r√©gulier/Rein droit",
"Contour r√©gulier/Rein gauche",
"Calcul R√©nal",
"Kyste",

# üîπ 7. Comorbidit√©s
"Personnels M√©dicaux/HTA",
"Personnels M√©dicaux/Diab√®te 2",
"Pathologies/R√©tinopathie hypertensive",
"Pathologies/R√©tinopathie diab√©tique"
]

# ================================
# TARGET
# ================================

target = "Stage de l'IRC"

# ================================
# EXTRACTION
# ================================

feat = df_filtered[features]
y = df_filtered[target]

print("Shape feat:", feat.shape)
print("Shape y :", y.shape)

feat

# ================================
# Affichage de la liste finale des features
# ================================

# feature_cols correspond aux colonnes utilis√©es pour X dans ton mod√®le
print("‚úÖ Nombre total de features :", len(feat.columns))
print("‚úÖ Liste finale des features :")
for i, col in enumerate(feat.columns, start=1):
    print(f"{i}. {col}")

# Afficher les 10 premi√®res lignes du dataset X
import pandas as pd

pd.set_option('display.max_columns', None)  # Affiche toutes les colonnes
pd.set_option('display.max_rows', 50)       # Affiche jusqu'√† 50 lignes (modifie selon besoin)

# Affichage
display(feat.head(50))  # Affiche les 50 premi√®res lignes du dataset

import numpy as np
import pandas as pd

def calcul_dfg_ckd_epi(sexe, age, creatinine_mg_L):
    """
    Calcule le DFG selon la formule CKD-EPI 2021 (sans facteur ethnique).

    Param√®tres :
    - sexe : 'F' ou 'M'
    - age : √¢ge en ann√©es
    - creatinine_mg_L : cr√©atinine en mg/L

    Retour :
    - DFG estim√© en mL/min/1.73m¬≤ (arrondi √† 2 d√©cimales)
    """

    # V√©rification des valeurs manquantes ou invalides
    if pd.isna(creatinine_mg_L) or pd.isna(age) or pd.isna(sexe):
        return np.nan
    if creatinine_mg_L <= 0 or age <= 0:
        return np.nan

    # Conversion mg/L ‚Üí mg/dL
    creatinine_mg_dL = creatinine_mg_L / 10

    # Conversion sexe en string
    sexe_str = str(sexe).upper()

    # Param√®tres selon le sexe
    if sexe_str == 'F':
        kappa = 0.7
        alpha = -0.241
        facteur_sexe = 1.012
    elif sexe_str == 'M':
        kappa = 0.9
        alpha = -0.302
        facteur_sexe = 1.0
    else:
        return np.nan  # sexe invalide

    # Formule CKD-EPI 2021
    dfg = (
        142
        * min(creatinine_mg_dL / kappa, 1) ** alpha
        * max(creatinine_mg_dL / kappa, 1) ** -1.200
        * (0.9938 ** age)
        * facteur_sexe
    )

    return round(dfg, 2)

df_filtered["dfg"] = df_filtered.apply(
    lambda row: calcul_dfg_ckd_epi(
        row["Sexe"],
        row["Age"],
        row["Cr√©atinine (mg/L)"]
    ),
    axis=1
)

display(df_filtered[["Sexe", "Age", "Cr√©atinine (mg/L)", "dfg"]].head(10))

# Retirer la ligne suppl√©mentaire dans df_filtered
df_filtered_clean = df_filtered.loc[df_filtered.index.isin(feat.index)]

# Ajouter la colonne dfg √† feat
feat["dfg"] = df_filtered_clean["dfg"].values

print("\nAper√ßu des 10 premi√®res lignes de feat :")
display(feat.head(10))

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder

# üîπ Copier le dataset
df_ml = df_filtered.copy()

# ================================
# 1Ô∏è‚É£ CIBLE : Stage de l'IRC
# ================================

# Nettoyage
df_ml["Stage de l'IRC"] = df_ml["Stage de l'IRC"].astype(str).str.strip()

# Supprimer valeurs manquantes
df_ml = df_ml[df_ml["Stage de l'IRC"].notna()]

# üö® Supprimer la valeur "0" (classe invalide)
df_ml = df_ml[df_ml["Stage de l'IRC"] != "0"]

# V√©rification avant encodage
print("Valeurs uniques AVANT encodage :")
print(df_ml["Stage de l'IRC"].unique())

# Encodage propre
le = LabelEncoder()
df_ml["Stage_de_l_IRC_Encoded"] = le.fit_transform(df_ml["Stage de l'IRC"])

# V√©rification finale
print("\nEncodage des classes :")
for i, class_ in enumerate(le.classes_):
    print(f"{class_} --> {i}")

# ================================
# 2Ô∏è‚É£ VARIABLES BINAIRES
# ================================

binary_vars = [
    "Sexe",
    "Personnels M√©dicaux/HTA",
    "Personnels M√©dicaux/Diab√®te 2",
    "Pathologies/R√©tinopathie hypertensive",
    "Pathologies/R√©tinopathie diab√©tique",
    "An√©mie",
    "Calcul R√©nal",
    "Kyste",
    "Contour r√©gulier/Rein droit",
    "Contour r√©gulier/Rein gauche"
]

for col in binary_vars:
    df_ml[col + "_Encoded"] = df_ml[col].map({
        "Oui": 1, "Non": 0,
        "F": 1, "H": 0
    }).fillna(0)

# ================================
# 3Ô∏è‚É£ VARIABLES ORDINALES
# ================================

echogen_mapping = [
    "Normale", "L√©g√®re", "Mod√©r√©e", "S√©v√®re",
    "Normal", "Hyper Echog√©nicit√©", "Hypo Echog√©nicit√©"
]

df_ml["Echog√©nicit√©_Encoded"] = OrdinalEncoder(
    categories=[echogen_mapping],
    handle_unknown='use_encoded_value',
    unknown_value=-1
).fit_transform(df_ml[["Echog√©nicit√©"]])

diff_mapping = ["Conserv√©e", "Diminu√©e", "Disparue"]

df_ml["Diff√©renciation des reins_Encoded"] = OrdinalEncoder(
    categories=[diff_mapping],
    handle_unknown='use_encoded_value',
    unknown_value=-1
).fit_transform(df_ml[["Diff√©renciation des reins"]])

df_ml["Score de Glasgow (/15)_Encoded"] = pd.to_numeric(
    df_ml["Score de Glasgow (/15)"], errors='coerce'
)

# ================================
# 4Ô∏è‚É£ VARIABLES LIPIDIQUES ET URINAIRES
# ================================

lipides_mapping = {"R√©duit":0, "Normal":1, "Normale":1, "Augment√©":2}

for col in ["Cholest√©rol Total", "Cholest√©rol HDL",
            "Cholest√©rol LDL", "Triglyc√©rides"]:
    df_ml[col + "_Encoded"] = df_ml[col].map(lipides_mapping)

diurese_mapping = {
    "Anurie":0,
    "Oligurie":1,
    "Non quantifi√©e":-1,
    "Pr√©serv√©e":2
}

df_ml["Diur√®se_Encoded"] = df_ml["Diur√®se"].map(diurese_mapping)

pu_mapping = {"-":0, "+":1}
df_ml["Tests Populaires/PU (g/24h)_Encoded"] = \
    df_ml["Tests Populaires/PU (g/24h)"].map(pu_mapping)

# ================================
# 5Ô∏è‚É£ VARIABLES NUM√âRIQUES
# ================================

numeric_vars = [
    "Age", "Poids (Kg)", "Taille (m)",
    "TA (mmHg)/Systole", "TA (mmHg)/Diastole",
    "Poul (bpm)", "Temp√©rature (C¬∞)",
    "Cr√©atinine (mg/L)", "Ur√©e (g/L)",
    "Glyc√©mie √† jeun (taux de Glucose)",
    "Hb (g/dL)", "Hte (%)",
    "Cholest√©rol Total_Encoded",
    "Cholest√©rol HDL_Encoded",
    "Cholest√©rol LDL_Encoded",
    "Triglyc√©rides_Encoded",
    "Diur√®se_Encoded",
    "Tests Populaires/PU (g/24h)_Encoded"
]

existing_numeric_vars = [col for col in numeric_vars if col in df_ml.columns]

scaler = StandardScaler()
df_ml[existing_numeric_vars] = scaler.fit_transform(
    df_ml[existing_numeric_vars]
)

# ================================
# 6Ô∏è‚É£ FEATURES & TARGET
# ================================

feature_cols = (
    [col + "_Encoded" for col in binary_vars] +
    ["Echog√©nicit√©_Encoded",
     "Diff√©renciation des reins_Encoded",
     "Score de Glasgow (/15)_Encoded"] +
    existing_numeric_vars
)

X = df_ml[feature_cols]
y = df_ml["Stage_de_l_IRC_Encoded"]

print("\n‚úÖ Shape X :", X.shape)
print("‚úÖ Shape y :", y.shape)
print("‚úÖ Colonnes utilis√©es :", X.columns.tolist())

X

# Retirer la ligne suppl√©mentaire dans df_filtered
df_filtered_clean = df_filtered.loc[df_filtered.index.isin( X.index)]

# Ajouter la colonne dfg √† X
X["dfg"] = df_filtered_clean["dfg"].values

print("\nAper√ßu des 10 premi√®res lignes de X :")
display(X.head(10))

# Supprimer la classe qui a seulement 1 observation
y_counts = y.value_counts()

# Garder uniquement classes avec au moins 2 √©chantillons
valid_classes = y_counts[y_counts >= 2].index

mask = y.isin(valid_classes)

X = X[mask]
y = y[mask]

print("Nouvelle distribution :")
print(y.value_counts())

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score



# ===========================
# 2Ô∏è‚É£ S√©parer Train / Test
# ===========================
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

xgb.fit(X_train, y_train)

y_pred = xgb.predict(X_test)

print(classification_report(y_test, y_pred))

from sklearn.model_selection import cross_val_score

scores = cross_val_score(xgb, X, y, cv=5, scoring='f1_macro')

print("F1 Macro CV :", scores)
print("Mean :", scores.mean())

import matplotlib.pyplot as plt

plt.figure(figsize=(10,8))
plt.barh(X.columns, xgb.feature_importances_)
plt.show()

import xgboost
print(xgboost.__version__)

from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric="mlogloss",
    early_stopping_rounds=20   # ‚úÖ ici maintenant
)

xgb.fit(
    X_train,
    y_train,
    eval_set=[(X_test, y_test)],
    verbose=False
)

print("Best iteration:", xgb.best_iteration)

# ==========================================
# 1Ô∏è‚É£ Importations
# ==========================================

import numpy as np
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay


# ==========================================
# 2Ô∏è‚É£ S√©paration Train / Test
# ==========================================

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)


# ==========================================
# 3Ô∏è‚É£ Mod√®le Final (96 arbres optimaux)
# ==========================================

xgb_final = XGBClassifier(
    n_estimators=96,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric="mlogloss"
)

# Entra√Ænement
xgb_final.fit(X_train, y_train)


# ==========================================
# 4Ô∏è‚É£ Pr√©dictions
# ==========================================

y_pred = xgb_final.predict(X_test)


# ==========================================
# 5Ô∏è‚É£ √âvaluation
# ==========================================

print("üìä Rapport de classification :\n")
print(classification_report(y_test, y_pred))

print("üìä Accuracy :", np.mean(y_pred == y_test))


# ==========================================
# 6Ô∏è‚É£ Matrice de confusion
# ==========================================

plt.figure(figsize=(8,6))
ConfusionMatrixDisplay.from_estimator(xgb_final, X_test, y_test)
plt.title("Matrice de Confusion - XGBoost Final")
plt.show()


# ==========================================
# 7Ô∏è‚É£ Feature Importance
# ==========================================

plt.figure(figsize=(10,8))
plt.barh(X.columns, xgb_final.feature_importances_)
plt.title("Feature Importance - XGBoost Final")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

import shap

# Explainer
explainer = shap.TreeExplainer(xgb_final)
shap_values = explainer.shap_values(X_test)

# Plot global importance
shap.summary_plot(shap_values, X_test)

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

y_test_bin = label_binarize(y_test, classes=np.unique(y))
y_score = xgb_final.predict_proba(X_test)

for i in range(y_test_bin.shape[1]):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"Classe {i} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Multi-class")
plt.legend()
plt.show()

from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# ‚úÖ S√©lection des features importantes
important_features = [
    "dfg",
    "Cr√©atinine (mg/L)",
    "Ur√©e (g/L)",
    "Temp√©rature (C¬∞)",
    "Cholest√©rol LDL_Encoded",
    "Cholest√©rol Total_Encoded"
]

X_imp = df_ml[important_features]
y_imp = df_ml["Stage_de_l_IRC_Encoded"]

# ‚úÖ S√©paration train / test
X_train, X_test, y_train, y_test = train_test_split(
    X_imp, y_imp, test_size=0.2, random_state=42, stratify=y_imp
)

# ‚úÖ Entra√Ænement XGBoost
xgb = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

xgb.fit(X_train, y_train)

# ‚úÖ Pr√©diction et √©valuation
y_pred = xgb.predict(X_test)

print("üîπ Matrice de confusion :")
print(confusion_matrix(y_test, y_pred))

print("\nüîπ Rapport de classification :")
print(classification_report(y_test, y_pred))

# ‚úÖ Feature importance pour ces 6 variables
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.barh(important_features, xgb.feature_importances_)
plt.title("Feature Importance - Variables cl√©s uniquement")
plt.show()

import pickle

from xgboost import XGBClassifier
import pickle
import pandas as pd

# üîπ 1Ô∏è‚É£ Tes 6 features importantes
important_features = [
    "dfg",
    "Cr√©atinine (mg/L)",
    "Ur√©e (g/L)",
    "Temp√©rature (C¬∞)",
    "Cholest√©rol LDL_Encoded",
    "Cholest√©rol Total_Encoded"
]

X_imp = df_ml[important_features]  # DataFrame des features
y_imp = df_ml["Stage_de_l_IRC_Encoded"]  # Cible

# üîπ 2Ô∏è‚É£ Cr√©er le mod√®le XGBoost avec les m√™mes hyperparam√®tres
xgb_imp = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# üîπ 3Ô∏è‚É£ Entra√Æner le mod√®le
xgb_imp.fit(X_imp, y_imp)

# üîπ 4Ô∏è‚É£ Sauvegarder le mod√®le strictement sur ces 6 features
filename = "xgb_model_6features.pkl"
with open(filename, "wb") as file:
    pickle.dump(xgb_imp, file)

print(f"‚úÖ Mod√®le enregistr√© dans {filename} avec les 6 features")

# Charger le mod√®le
with open("xgb_model_6features.pkl", "rb") as file:
    xgb_loaded = pickle.load(file)

# Cr√©er un DataFrame de test avec exactement les 6 features
X_test_new = pd.DataFrame([
    [60, 1.2, 0.5, 36.5, 1, 2],
    [45, 1.8, 0.7, 37.0, 2, 3],
    [70, 1.0, 0.4, 36.8, 1, 2]
], columns=important_features)

# Faire la pr√©diction
y_pred_new = xgb_loaded.predict(X_test_new)
print("‚úÖ Pr√©dictions :")
print(y_pred_new)

y_pred_new = [5, 5, 5]  # ‚úÖ correct
print(y_pred_new)

from google.colab import files

# üîπ T√©l√©charger le fichier vers ton PC
files.download("xgb_model_6features.pkl")

